# -*- coding: utf-8 -*-
"""Data Mining_Project_Group11_Kolaparthi, Deevi, Vadala

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r5xle_f67hUr_TwaEXJrxXhV21hUaoE2

#Importing required libraries
"""

# Import necessary libraries for data manipulation, visualization, and machine learning
import pandas as pd  # Data manipulation and analysis
import numpy as np  # Numerical computing
import warnings  # Handle warnings
import matplotlib.pyplot as plt  # Basic plotting
import plotly.express as px  # Interactive plotting
import seaborn as sns  # Statistical data visualization

# Import machine learning related modules
from sklearn.model_selection import train_test_split  # Split data into training and testing sets
from sklearn.linear_model import LinearRegression  # Linear regression model
from sklearn.ensemble import RandomForestRegressor  # Random Forest regression
from sklearn.tree import DecisionTreeRegressor  # Decision Tree regression
from sklearn.neighbors import KNeighborsRegressor  # K-Nearest Neighbors regression
import lightgbm as lgb  # Light Gradient Boosting Machine
from sklearn.ensemble import GradientBoostingRegressor  # Gradient Boosting regression
from sklearn import metrics  # Model evaluation metrics
from sklearn.metrics import mean_squared_error, r2_score  # Specific metrics for regression

## Import Google Colab drive mounting utility
from google.colab import drive
drive.mount('/content/drive')

"""#Loading the dataset"""

data = pd.read_csv('/content/drive/MyDrive/diamonds.csv') #Read the diamond dataset from Google Drive
data.head() ## Display first few rows of the dataset

description = pd.read_excel('/content/drive/MyDrive/Data Dictionary.xlsx') #Read the description sheet about the data from Google Drive
description

data.info() ## Get comprehensive information about the dataset

data.dtypes
#Check the data types of columns
#Will do one-hot encoding on categorical variables if needed

"""#Data Pre-processing

###Removing the unwanted column

**We can see that there's an unwanted column in the dataset. We can remove the column**
"""

# Remove the first unnamed column prevents redundant index column from interfering with analysis
data = data.drop(columns=['Unnamed: 0'])
data

"""###**Getting the descriptive statistics of the dataset**"""

data.describe() ## Generate descriptive statistics for numerical columns

"""###Checking for null values"""

data.isnull().sum() #Check for missing values in each column

"""There are no values in the dataset."""

print(data.shape)

# Data Cleaning: Remove rows with zero or negative dimensions
# This ensures only physically possible diamond dimensions are kept
# x, y, z represent diamond's dimensions (length, width, depth)

data = data[(data['x'] > 0) & (data['y'] > 0) & (data['z'] > 0)]
print(data.shape)

"""#EDA

###a. Box plot for Numerical variables
"""

numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns

# Create subplot grid for distributions
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols, 1):
  plt.subplot(3, 4, i)
  sns.histplot(data[col], kde=True)
  plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

"""###b. Distribution of Traget variable (Price)"""

plt.hist(data['price'], bins=20)
plt.title('Distribution of gem prices in data set')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

figure = px.scatter(data_frame = data, x="carat",
                    y="price",
                    color= "cut", trendline="ols")
figure.show()

fig = px.box(data, x="cut",
             y="price",
             color="color")
fig.show()

figure = px.box(data_frame = data, x="clarity",
                    y="price", color= "cut")
figure.show()

"""###Correlation Matrix"""

numerical_data = data.select_dtypes(include=[np.number]) #Taking numerical variables to get correlation between the variables
correlation_matrix = numerical_data.corr() #Creating the coreelation matrix
correlation_matrix

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Diamonds Dataset')
plt.show()

"""

```
# This is formatted as code
```

###One-hot encoding Categorical variables (Cut, Clarity and Color) into numerical values"""

cut_mapping={"Fair":1,"Good":2,"Very Good":3,"Premium":4,"Ideal":5}
data['cut'] = data['cut'].map(cut_mapping)

clarity_mapping={"I1":1,"SI2":2 ,"SI1":3 ,"VS2":4 , "VS1":5 , "VVS2":6 , "VVS1":7 ,"IF":8}
data['clarity'] = data['clarity'].map(clarity_mapping)

color_mapping={"D":1,"E":2,"F":3,"G":4,"H":5,"I":6,"J":7}
data['color'] = data['color'].map(color_mapping)

"""###Multiplying X(length), Y(Width) and Z(Height) into a single column as Size"""

data['size'] = data['x'] * data['y'] * data['z']

data = data.drop(columns=['depth', 'table']) #Removing Depth and table columns as they are negatively correlated with predictor.

data.head()

"""#Model Building and Evaluation

##Data Splitting
"""

#Seperating features and target variable
X = data.drop(['price'], axis=1)
y = data['price']

#Splitting data into training and test data
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.25,  #25% test data
                                                    random_state=42) #Reproducibility

"""##Linear Regression"""

#Linear Regression
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)
y_pred_lr = model_lr.predict(X_test)
mse_lr = mean_squared_error(y_test, y_pred_lr)
rmse_lr = np.sqrt(mse_lr)
r2_lr = r2_score(y_test, y_pred_lr)

print(f"Linear Regression = R2: {r2_lr}, RMSE: {rmse_lr}")

"""##Decision Tree"""

# Decision Tree
model_dt = DecisionTreeRegressor(random_state=42)
model_dt.fit(X_train, y_train)
y_pred_dt = model_dt.predict(X_test)
mse_dt = mean_squared_error(y_test, y_pred_dt)
r2_dt = r2_score(y_test, y_pred_dt)
rmse_dt = np.sqrt(mse_dt)

print(f"Decision Tree = R2: {r2_dt}, RMSE: {rmse_dt}")

"""##KNN"""

# KNN
model_knn = KNeighborsRegressor(n_neighbors=5)
model_knn.fit(X_train, y_train)
y_pred_knn = model_knn.predict(X_test)
mse_knn = mean_squared_error(y_test, y_pred_knn)
r2_knn = r2_score(y_test, y_pred_knn)
rmse_knn = np.sqrt(mse_knn)

print(f"KNN = R2: {r2_knn}, RMSE: {rmse_knn}")

"""##XG Boost"""

# XGBoost
model_xgb = GradientBoostingRegressor(random_state=42)
model_xgb.fit(X_train, y_train)
y_pred_xgb = model_xgb.predict(X_test)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mse_xgb)

print(f"XGBoost = R2: {r2_xgb}, RMSE: {rmse_xgb}")

"""##LightGBM"""

#LightGBM model
model_lgb = lgb.LGBMRegressor(random_state=42)
model_lgb.fit(X_train, y_train)
y_pred_lgb = model_lgb.predict(X_test)
mse_lgb = mean_squared_error(y_test, y_pred_lgb)
r2_lgb = r2_score(y_test, y_pred_lgb)
rmse_lgb = np.sqrt(mse_lgb)

print(f"LightGBM = R2: {r2_lgb}, RMSE: {rmse_lgb}")

"""##Random Forest"""

# Random Forest
model_rf = RandomForestRegressor(random_state=42)
model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)

print(f"Random Forest = R2: {r2_rf}, RMSE: {rmse_rf}")

"""#Results"""

#Store model performance results
results = {
    'Model': ['Linear Regression', 'Decision Tree', 'KNN', 'XGBoost', 'LightGBM', 'Random Forest'],
    'R2': [r2_lr, r2_dt, r2_knn, r2_xgb, r2_lgb, r2_rf],
    'RMSE': [rmse_lr, rmse_dt, rmse_knn, rmse_xgb, rmse_lgb, rmse_rf]
}

results_df = pd.DataFrame(results)
results_df